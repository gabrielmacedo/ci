---
title: Cronografia da Ciência da Informação
description: Disciplina ministrada no programa de pós-graduação do IBICT-UFRJ
professor: Lillian Maria Araujo de Rezende Alvares
tags: 
  - information science
  - ibict
  - ufrj
  - 2023
  - chronography
---


# Cronografia da Ciência da Informação

Professora: Lillian Maria Araujo de Rezende Alvares ([site](http://lillianalvares.fci.unb.br/), [Lattes](http://lattes.cnpq.br/5541636086123721), [ORCID](https://orcid.org/0000-0002-8920-0150), [Figshare](https://figshare.com/authors/Lillian_Maria_Araujo_de_Rezende_Alvares/11553475), [OSF](https://osf.io/3hkrc/))

- [Ementa da disciplina](http://lillianalvares.fci.unb.br/cronografia)


## Aulas

### Aula 01 (23/03/2023)


#### Notas

Cronografia - se refere ao estudo da ordem dos eventos históricos. É uma linha do tempo onde colocamos diversas camadas.

A duração entre os eventos
A relação entre os eventos
Tempo entre um e outro

Se refere a ordem e ao tempo dos eventos históricos da ciência da informação. É uma pesquisa histórica.


#### Trabalho 

- Trabalho com 1500 palavras no final de cada módulo. Conjunção dos temas dentro de cada módulo.

Resumo dos temas abordando a cronografia.

- Quais foram as grandes abordagens? Como elas ocorreram? Quais as consequências da abordagem? (Será possível usar na segurança da aviação?)

- Cronografia pode ser dos autores, dos conteúdos e das trajetórias/consequências do conteúdo.

Para o trabalho: o que alimentou Shannon-Weaver (teoria quantitativa), o que vem antes e o que vem depois.

- Verificar possibilidade de rodar os textos numa linha do tempo. Caso funcione, rodar os outros conteúdos da cronografia.

Fonte: [Christopher Gaudreau](https://www.sutori.com/en/item/theories-based-on-content-and-messaging-shannon-weaver-model-of-communication)
```
Teorias baseadas em conteúdo e mensagens: o modelo de comunicação de Shannon Weaver enfatizou a importância do significado da mensagem versus a entrega da informação.

Shannon era um matemático americano, enquanto Weaver era um cientista. A teoria matemática mais tarde veio a ser conhecida como modelo de comunicação de Shannon Weaver ou “mãe de todos os modelos”.

A teoria da informação estuda a quantificação, armazenamento e comunicação da informação . Foi originalmente proposto por Claude E. Shannon em 1948 para encontrar limites fundamentais no processamento de sinais e operações de comunicação, como compressão de dados, em um artigo histórico intitulado "A Mathematical Theory of Communication".

O trabalho de Shannon e Weaver provou ser valioso para engenheiros de comunicação ao lidar com questões como a capacidade de vários canais de comunicação em 'bits por segundo'. Contribuiu para a ciência da computação. Isso levou a um trabalho muito útil sobre redundância na linguagem. E ao tornar a 'informação' 'mensurável' deu origem ao estudo matemático da 'teoria da informação'.

A tecnologia de comunicação impulsiona a entrega de informações com um novo foco na mensagem (comunicação). O verdadeiro nascimento da moderna teoria da informação pode ser rastreado até a publicação em 1948 de “A Mathematical Theory of Communication” de Claude Shannon no Bell System Technical Journal.

Um passo fundamental no trabalho de Shannon foi sua percepção de que, para se ter uma teoria, os sinais de comunicação devem ser tratados isoladamente do significado das mensagens que eles transmitem. Essa visão contrasta fortemente com a concepção comum de informação, na qual o significado tem um papel essencial. Shannon enfocou o problema de como codificar melhor a informação que um remetente deseja transmitir. Ele também usou ferramentas na teoria da probabilidade, desenvolvidas por Norbert Wiener. Eles marcaram os estágios nascentes da teoria da comunicação aplicada naquela época.
```

#### Textos

BUCKLAND, M. K. Information as Thing. Apresentadora: **Lidianne Albernaz** (ela estuda taxonomia e web semântica no doutorado).

- Dado: coisa processada com um objetivo; tende a se referir a iformação numérica.
- Texto: descreve a linguagem natural utilizada; falada, escrita, sinais.
- Documento: um objeto com contém texto (texto e documento também devem incluir outras formas de encapsular - sons, imagens, etc).


Pegar prints do dia 23/03/2023.

Determinar se algo é considerado informação ou não dependerá do conceito. Deverá haver algum nível de consenso.

_O progresso das tecnologias de informação amplia o escopo de criação e uso da informação como coisa._ Considera a reutilização da informação, a criação de cópias dos documentos. A codificação, interpretação e resumo são processos que criam representação da informação quanto evidência; no entanto podem apresentar viéses e distorções. Assim, a *informação enquanto coisa é situacional* e possui algum nível de consenso.

O fato dos _sistemas de informação_ lidarem com informação enquanto coisa é o motivo principal dessa compreensão. A armazenagem da informação e sua recuperação não é o conhecimento em si. 

A informação tem o poder de modificar crenças, uma vez que ela é situacional, ligada ao contexto e a percepção dessa informação pode mudar a compreensão de quem se informa.

> Pegar a apresentação da colega e a de Lillian

Autores: Norbert Wiener ([Q178577](https://www.wikidata.org/wiki/Q178577))

<img width="509" alt="image" src="https://user-images.githubusercontent.com/20596966/227236575-b40f6ec5-27a9-4196-b2c0-c74f28b95132.png">

### Aula 02 (30/03/2023)

Shannon ([Q2277038](https://www.wikidata.org/wiki/Q2277038), [Reasonator](https://reasonator.toolforge.org/?q=Q2277038))

Weaver


Teoria matemática da informação: `A informação é a mensagem`

<img width="512" alt="image" src="https://user-images.githubusercontent.com/20596966/227232371-3bfd49f9-e8d2-4469-8ac5-89f3866f4e21.png">

Informação como coisa

<img width="510" alt="image" src="https://user-images.githubusercontent.com/20596966/227233980-236b8d27-6201-4f2a-8eea-66a469369c42.png">

fgouveia@gmail.com - enviar e-mail para participação como ouvinte nas aulas sobre as ferramentas de cientometria/bibliometria

#### Apresentação sobre SABAH (Gabriel)

> inserir meus slides da apresentação

#### Apresentação do texto de Klein (Nelson)

#### Autores principais

Karl Popper:
- teorias científicas podem ser testadas e refutadas empiricamente.

Thomas Gieryn:
- _Boundary work_: demarcação de campos científicos passa por disputas de poder e tentativas de estabelecimento de monopólios; não apenas uma discussão técnica.

Shannon-Weaver (1948):
- teoria quantitativa da comunicação, empresta termos a outros campos para adaptar o conceito de informação. Conceitos como entropia (desordem) do sistema, quanto maior a entropia (maior a incerteza) -> maior a informação em termos de quantidade de dados necessários.

Norbert Weiner (1948):
- cybernetics, controle e comunicação no animal e na máquina, a sociedade só pode ser compreendida através do estudo das mensagens, por meio dos seus emissores e receptores.
- teoria das mensagens, teoria probabilística da linguagem, 1) tranmissão da mensagem; 2) linguagem; 3) estudo da forma de controle entre animal, homem e máquina. Perspectiva de comunicação e controle entre homem-homem e homem-máquina.

#### Contribuições à Teoria da Informação

- sistemas informacionais baseados em aspectos não semânticos em **Shannon-Weaver**.
- base teórica para a Ciência da Informação, busca por métodos e por legitimidade, rigor matemático e aplicação em sistemas de recuperação da informação.
- uso abrangente da teoria de Shannon-Weaver, bibliotecários e CIs tentam apropriar o conceito para o campo e tomá-lo como uma explicação científica; apresentação de metáfora para tópicos mais abrangentes; ferramenta matemática para desenhar sistemas de recuperação da informação.

#### Problemas da aplicação da teoria da informação de Shannon-Weaver

> pesquisadores ampliaram o conceito para trabalhar problemas semânticos, no entanto SW trabalha a teoria em termos de transmissão e de significado, apesar de reconhecer a existência de outros níveis.

Problemas:
- 1º: problema dos 3 níveis de comunicação (técnico-quantitativo-precisão, semântico-qualitativo-verdade e pragmático-impacto-eficiência);
- 2º: equivalência matemática entre entropia e informação.

Robert Fairthorne:
- leva a Teoria da Informação a bibliotecários nos anos 1950. A teoria da informação era necessária, mas insuficiente. Ideia de complementar a teoria da informação de SW. Quem emite sinais para quem?
- tentativa de substituir SW por modelos próprios da área.

Thomas Minder: 
- vê como inadequada a teoria de SW; não mais apenas como insuficiente.

> Quando as aplicações das teorias da informação começam a falhas, ela é segmentada para manter a validade dos seus conceitos em alguns campos. A CI e alguns teóricos da informação adotam estratégias (resposta a editoriais, sátiras, etc) para disciplinar as aplicações da teoria. O uso em outras áreas é então desencorajado.

> Lillian: no nascimento da CI, a teoria matemática dá segurança à área uma vez que há uma identificação da teoria com a ideia buscada de **recuperação da informação**, seminal na constituição do campo.

> Conceito de entropia em SW `chatGPT`: quanto mais imprevisível é uma fonte, maior é a sua entropia. A entropia pode ser interpretada como a quantidade média de informação contida em cada símbolo da fonte de informação. Quanto mais entropia uma fonte de informação tiver, maior será a quantidade média de informação necessária para representar cada símbolo da fonte. A entropia é medida em bits, e é representada pela fórmula:

`H(X) = -Σ p(xi) log2 p(xi)`

> `chatGPT` a entropia é uma medida da quantidade de informação que uma fonte de informação contém, e é calculada com base na probabilidade de ocorrência de cada símbolo na fonte. Quanto mais imprevisível é a fonte, maior é a sua entropia.

| Conceito | Características |
| --- | --- |
| Entropia | A entropia é uma medida da incerteza ou imprevisibilidade de uma fonte de informação. |
| Cálculo | A entropia é calculada com base na probabilidade de ocorrência de cada símbolo na fonte de informação. |
| Unidade | A entropia é medida em bits. |
| Significado | A entropia pode ser interpretada como a quantidade média de informação contida em cada símbolo da fonte de informação. |
| Relação com a aleatoriedade | Quanto mais aleatória ou imprevisível é a fonte de informação, maior é a sua entropia. |
| Relação com a redundância | Quanto mais redundante é a fonte de informação, menor é a sua entropia. |
| Utilização | A entropia é utilizada em diversas áreas, como compressão de dados, criptografia e reconhecimento de padrões. |
| Limitações | A entropia não leva em consideração o significado ou contexto da informação, apenas a sua frequência de ocorrência. |
| Aplicação na teoria da informação | A entropia é um dos principais conceitos da teoria da informação de Shannon-Weaver, juntamente com a redundância e a capacidade de canal. |

- Entropia Negativa `chatGPT`:

```
O conceito de "entropia negativa" é uma ideia que pode parecer contraditória à primeira vista, mas que possui um significado importante na teoria da informação.

A entropia é definida como uma medida da incerteza ou da aleatoriedade de uma fonte de informação, e é sempre uma quantidade não negativa. Isso significa que a entropia nunca pode ser menor do que zero. No entanto, em alguns contextos, é possível obter uma entropia "negativa".

Uma entropia negativa é uma medida da informação que é mais "organizada" ou menos "aleatória" do que o esperado de uma fonte de informação. Em outras palavras, uma entropia negativa indica que a fonte de informação é menos imprevisível do que o esperado.

Isso pode ocorrer quando há uma estrutura subjacente ou um padrão na fonte de informação que pode ser explorado para reduzir a incerteza. Por exemplo, em uma sequência de DNA, a presença de certas sequências pode indicar a presença de outras sequências próximas, reduzindo assim a incerteza.

Embora a entropia negativa pareça contraditória, ela tem aplicações importantes na teoria da informação, especialmente em campos como a criptografia, onde a detecção de padrões pode ajudar a quebrar a criptografia. No entanto, a entropia negativa é um conceito relativamente raro e é importante lembrar que a entropia geralmente não pode ser menor do que zero.
```

- O conceito de entropia na Teoria da Informação `chatGPT`: O conceito de entropia na teoria da informação foi introduzido por Claude Shannon em seu artigo seminal "A Mathematical Theory of Communication", publicado em 1948. Shannon usou o termo "entropia" para descrever a quantidade de informação contida em uma mensagem ou fonte de informação. Ele mostrou que a entropia pode ser calculada com base na probabilidade de ocorrência de cada símbolo na fonte de informação e que a entropia máxima ocorre quando todos os símbolos têm a mesma probabilidade de ocorrência.

Outro autor importante que trabalhou o conceito de entropia na teoria da informação foi Warren Weaver, que colaborou com Shannon no desenvolvimento da teoria. Weaver foi responsável por popularizar o conceito de entropia em outros campos além da comunicação, como a biologia e a economia.

Além disso, outros pesquisadores, como Norbert Wiener e John von Neumann, também contribuíram para o desenvolvimento da teoria da informação e da entropia. Wiener aplicou o conceito de entropia à teoria dos sistemas e à cibernética, enquanto von Neumann utilizou a entropia na teoria dos jogos e em outros campos da matemática.

Em resumo, Claude Shannon é amplamente considerado o "pai" da teoria da informação e foi o primeiro a trabalhar o conceito de entropia dentro desse contexto, mas outros autores, como Warren Weaver, Norbert Wiener e John von Neumann, também contribuíram para o desenvolvimento da teoria.

#### Professora Lillian

- Teorias **quantitativas** da informação e teorias **qualitativas** da informação
- Conceitos de entropia na ciência da informação
- Neumann sugere a Shannon o conceito de entropia para a "informação que faltava", nesse sentido a "entropia da informação" surge.

##### Diversas teorias quantitativas da informação:


- Fisher (1925): informação no campo da probabilidade.
- Hartley (1928): transmissão da informação quanto a seleção de um conjunto de elementos e o que deve ser transportado para dar validade ao significado da informação.
- Shannon-Weaver (1948): teoria matemática da informação, teoria da transmissão da Comunicação, Teoria de Shannon-Weaver. Busca os limites fundamentais para o transporte da informação pelos canais. Na exposição de 1949 já apresenta 3 camadas - técnica, semântica (significado da verdade) e pragmática (influência relatica ao impacto no comportamento humano). É uma **teoria da mensagem associada à uma certa quantidade de informação**. O objetivo é que a informação chegue do outro lado do canal.

Teorias quantitativas da informação `chatGPT`: 

| Teoria                        | Autor(es)                            | Ano    | Conceito Principal Abordado         | Descrição                                                                                                                                                                                                                                                       |
|-------------------------------|--------------------------------------|--------|------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Teoria Matemática da Comunicação | Claude Shannon                      | 1948   | Entropia                           | A teoria define a quantidade de informação transmitida em uma mensagem como sendo proporcional à entropia, que é uma medida da incerteza ou desordem da fonte de informação.                                                                                                                                                 |
| Teoria da Complexidade Computacional | Stephen Cook, Leonid Levin         | 1971   | Complexidade Computacional         | A teoria estuda a complexidade de problemas computacionais e classifica-os de acordo com a dificuldade em resolvê-los. A complexidade é medida em termos de tempo e espaço necessários para resolver um problema, e é usada para determinar a eficiência de algoritmos.       |
| Teoria da Informação Quântica  | Stephen Wiesner                      | 1983   | Estados Quânticos                  | A teoria estuda a transmissão de informações usando estados quânticos em vez de bits clássicos. Os estados quânticos têm propriedades que permitem a transmissão de informações de forma mais segura e eficiente do que os bits clássicos.                            |
| Teoria da Codificação de Canal | Robert Gallager                      | 1963   | Codificação de Canal               | A teoria estuda a codificação de informações para transmissão em canais de comunicação que sofrem interferência e ruído. Ela se concentra em criar códigos que permitem a detecção e correção de erros, de modo a garantir a integridade das informações transmitidas.           |
| Teoria da Compressão de Dados | David Huffman                        | 1952   | Códigos de Huffman                 | A teoria estuda a compressão de dados por meio da criação de códigos que representam as informações de forma mais eficiente. O algoritmo de Huffman é um exemplo de algoritmo de compressão de dados que usa códigos de tamanho variável para representar informações de forma mais compacta. |
| Teoria da Informação Semântica | John F. Sowa                         | 1984   | Significado das Palavras            | A teoria studa a representação de informações em termos de seus significados semânticos. Ela se concentra em como as palavras e símbolos são usados para representar e comunicar informações de forma eficaz.                                                           |
| Teoria da Informação Visual    | Rudolf Arnheim                       | 1969   | Percepção Visual                   | A teoria estuda a transmissão de informações visuais e como as pessoas percebem e interpretam imagens. Ela se concentra em como as formas, cores e outros elementos visuais podem ser usados para comunicar informações de forma eficaz.                                   |
| Teoria da Decisão              | Leonard Savage, John von Neumann    | 1954   | Teoria da Decisão                  | A teoria estuda como as pessoas tomam decisões em situações de incerteza e risco. Ela se concentra em como as pessoas avaliam as alternativas disponíveis e escolhem a melhor opção com base em suas preferências e no valor que atribuem a cada resultado possível. |


> Lillian: a teoria de Shannon-Weaver é o divisor de águas entre o mundo analógico e o mundo digital.

- Assistir: "The bit player"

<iframe width="560" height="315" src="https://www.youtube.com/embed/CCrpgUM_rYc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>

- Kolmogorov: incompressividade, aleatoriedade e computabilidade. É comparada em grandeza com a teoria de Shannon.

- Teoria quântica da informação: estatística da informação no mundo quâtico.

##### Teorias qualitativas da informação

- Yehoshua Bar-Hillel, Rudolf Carnap, 1958: Teoria da informação semântica: o significado é a essência da informação. Teoria _Fraca_ da Informação Semântica; Teoria Clássica da Informação Semântica. Uso na computação linguística, Tradução automática, Recuperação da informação. Baseada na teoria da Semiótica de Pierce (não há informação nova-novidade se o que você recebe já é sabido-conhecido; ao receber a nova informação o processo de se conectar com o que já está presente no repertório é o que produz um novo conhecimento - o falante e o ouvinte precisa ter algo em comum e uma experiência real entre ambos; a informação semântica é **conteúdo**, é o conjunto de mundos possíveis excluídos da sentença; Conteúdo = informação semântica = mundos excluídos da sentença -- Intenção = Proposição = Mundos incluídos na sentença.

- Donald Mackay, 1969: Teoria Qualitativa da Teoria Quantitativa da Informação. A informação entá ligada à representação e símbolos, a medição dos símbolos determina a quantidade de mudança

- Doede Nauta, 1972: abordagem Semiótica e Cibernética. Informação está disponível, a compreensão depende de uma harmonia entre **informação - significado - representação**.

- Teoria da Informação Pragmática: também baseada na semiótica de Pierce; Repertório da Fonte - Repertório Comum - Repertório do Receptor; Se todos os repertórios forem idênticos, não se traz informação; Se todos os repertórios forem diferentes, não há diálogo (interseção); Há uma possibilidade de troca se os repertórios se tangenciam (há a possibilidade de novidade); Informação e Originalidade.

- Teoria da Informação Fortemente Semântica: a teoria semântica de Bar-Hillel e Carnap é um modelo fraco de explicação da verdade; **tese da veracidade** para diferenciar _informação significativa_ e _informação verdadeira_; Informação Fortemente Semântica (verdadeira); Teoria de Rede, Tanto informação verdadeira quanto informação falsa carregam informação.

"ALVARES, 2022. Análise histórica e epistemológica do vocábulo informação e de teorias que subsidiam os fenômenos relacionados a gestão da informação."


### Aula 03 (06/04/2023)

#### Apresentação colega Tereza

- Uso do google trends para avaliar a propagação dos termos envolvidos

> KARVALICS, L. Z. Information society: what is it exactly? The meaning, history and conceptual framework of an expression. Information Society: From Theory To Political Practice, v. 29, 2007.

- O autor busca capturar o conceito do que é **sociedade da informação**

Sociedade da Informação

Na virada do milênio, já era um termo usado em context politico, de negócios,marketing e no dia a dia.

- O conceito se desenvolveu separadamente do escopo empírico do objeto (a sociedade da informação em si)

- A expressão se tornou polissêmica. 

- os estudos de SI apenas no fim do milênio.

1961 - Kisho Kurokawa (arquiteto), em conversa
com Tudao Umesao (historiador e
antropologista)

1964 - Titulo de um estudo de Jiro Kamishima.
Mas quem deu o título foi o editor
Michiko Igarashi (Sociology in
Information Society)

1968 e 1969 - Três livros sobre o tema

1971 - Dicionário de Sociedade da Informação.

Yoneji Masuda, usou o termo em uma conferência.
1970
MAS ANTES DISSO...
A literatura em inglês usava outras expressões: Sociedade pós-industrial

Criado em 1914. Aumento do 3º setor na economia. Automação na tecnologia. Trabalho "mental" X Trabaho "braçal"

Vários termos proliferaram.
- 1950 a 1980 Termo guarda-chuva.

- White collar revolution. Se espalhou na década de 1950.

<img width="699" alt="image" src="https://user-images.githubusercontent.com/20596966/230382302-35ef46dd-95d5-442c-9697-892b515506d7.png">

A Sociedade do Conhecimento seria melhor do que a Sociedade da
Informação?

A Sociedade do Conhecimento seria uma parte da que a Sociedade da Informação?

Os conceitos "conhecimento" e "informação" são ambíguos.

As Ciências Sociais não definiram critérios para informação e conhecimento.

O modelo matemático-estatístico não* considera a qualidade da informação.

Os conhecimentos da biblioteconomia são válidos para uma área restrita.

Machlup, em 1962, diz que informação e conhecimento são a mesma qualidade.

"Aqueles que se interessam pela natureza da informação contribuem muito mais do que os que um usam um plágio da inútil fórmula de SW".

<img width="656" alt="image" src="https://user-images.githubusercontent.com/20596966/230383260-5de2887b-2519-4842-b5d8-1dac86c054e0.png">

Usando modelos em vez de definições...
Três divisões clássicas:
1. Daniel Bell: três períodos e nove aspectos de fases sócio-históricas.
Table: Dimensions of the information society according to Daniel Bell
- 1. Pre-industrial -> 2. Industrial -> 3. Post-industrial 

2. Yoneji Masuda
Table: Comparison of the characteristics of the industrial and information society by Yoneji Masuda
- industrial society -> information society

3. Schemet and Curtis: seis categorias (bens, indústria, trabalho e também interconectividade e mídia)

O autor faz uma síntese:

Tabela sintética: Retoma a ideia de SI como um termo guarda-chuva e tenta estabelecer indicadores para avaliar as sociedades. Constrói o modelo sob critério materiais para definir em que estágio está uma dada sociedade frente ao conceito de _sociedade da informação_.

<img width="647" alt="image" src="https://user-images.githubusercontent.com/20596966/230385041-7fdc7081-7ee2-4415-95be-22dddbe3face.png">

Três áreas de estudo sobre Sociedade da Informação

1 - grande narrativa ou análise no nível **macro-teoria** da civilização como contexto. Socialmente mais abrangente, faz perguntas mais amplas.

2 - pequena narrativa ou análise no nível **meso-teoria** do desenvolvimento como como contexto. "quais estágios, modelos e tipos existem dentro do desenvolvimento da
sociedade da informação e quais as regras pelas quais ela é regida".

3 - mini narrativa, no nível **micro-prática* e reflexão. "Fatias menores da realidade", uso prático. As narrativas não têm limites claros e, com frequência, estão interrelacionadas.

#### Apresentação colega Willian França

> LUKINA, N. P.; SAMOKHINA, N.N. Revisiting the distinctive features of the information society’s technological structure. Review of European Studies, v. 7, n. 7, 2015.

- Estudos como os de Beniger apontam que existiam, até 2009 (ano daquele
estudo) ao menos 70 definições sobre o termo "Sociedade da Informação",
embora o estudo anterior de Zins (2007) já apontasse mais de 130 definições
para o mesmo.

- E muitas nomeações para a sociedade moderna, sendo as mais frequentes:
**sociedade criativa**, **sociedade do conhecimento** e **sociedade do não
conhecimento**.

- O auge das pesquisas so re o ema, porém, foi entre os anos 1960 e 1980, tendo, de acordo com os autores, estado atualmente à margem das pesquisas sociais, filosóficas e sociológicas.

- Os autores abordam os impactos dos avanços tecnológicos sobre os fenômenos sociais.

- Também argumentam quanto a insuficiência das abordagens tecnocráticas isoladas para entender a natureza da sociedade da informação.

- Daí a necessidade de "reforçar sua própria base conceitual através do aperfeiçoamento de métodos de análise de categorias", o que também constitui um objetivo do artigo.

- Se empenham na identificação dos "componentes sociais, culturais e antropológicos da a estrutura tecnológica da sociedade da informação" e como esses componentes moldam as características-chave deste tipo de sociedade.

> Interesse em reduzir a pulverização conceitual e a permissividade - em caráter de panáceia

- What is "Technological Structure"?

- Teve origem entre economistas norte-americanos e da Europa ocidental, atrelada à chamada economia evolutiva (um sistema de relações e interconexões tecnológicas, industriais, comerciais e sociais).

- Essa economia se caracteriza justamente na transição de uma estrutura tecnológica para outra. (Freeman & Soete, 1997; Freeman & Louca, 2002; Nelson & Winter, 1982)

- Características distintivas:
- Saber se esse tipo de sociedade representa um modelo estruturalmente
novo de sistema social ou sucede o industrialismo e o pós-industrialismo com
todas as suas características e contradições.
- Os autores definem como característica "a sociedade da informação ser
considerada como uma sociedade industrial baseada na informação
condicionada pela economia de mercado".

- tecnologia aprimorada, de ciência avançada, de educação universal e de liberdades individuais.
- Orientação fundamental para o progresso, padronização, unificação e universalização.
- Otimismo tecnológico, transcendência dos avanços da tecnologia em todos os ambientes sociais, atitudes positivas em relação à informação e tecnologias de comunicação como fator definidor do progresso social.

Vantagem da sociedade da informação sobre a sociedade industrial:

- Novas formas de interação, apoiadas na informação e na comunicação por meio de tecnologias.

Outras características: A estrutura tecnológica da sociedade da informação é representada pela microeletrônica, software, robótica e novos materiais.

Partindo da premissa de que a estrutura tecnológica da sociedade da informação se explicita em "ondas", estaríamos vivendo a sexta onda (de 2010 a 2050).

Esta sexta onda da estrutura tecnológica incorpora campos como biotecnologia, nanotecnologia, sistemas de inteligência artificial, redes globais de informação e sistemas integrados de transporte de alta velocidade.

Isto é: a transformação do conhecimento baseado em ciência para o desenvolvimento de inovações para gerar novas produções e novos lucros para a economia.

Programada para trabalhar o direção ao individualismo e ao subjetivismo.

Conclusões: 

> Estrutura tecnológica como critério distintivo

Uma das características definidoras da estrutura tecnológica da sociedade da informação é revelada em sua intrusão do padrão técnico e tecnológico na natureza humana, iniciando mudanças qualitativas nesta última.

Tentar estabelecer as características distintivas da sociedade da informação com o foco principal em sua estrutura tecnológica são uma ilustração da eficácia da conceituação como um processo contínuo de aprofundar a
compreensão do objeto de estudo.



## Referências para Cronografia

- [The History and Historiography of Information Science: Some Reflections](https://www.researchgate.net/publication/222489196_The_History_and_Historiography_of_Information_Science_Some_Reflections/comments)January 1996. Information Processing and Management 32(1):3-17. DOI: 10.1016/0306-4573(95)00046-J Source dx.doi.org. W. Boyd RaywardW. Boyd Rayward
